{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Install packages"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# !pip3 install numpy\n",
    "# !pip3 install azure-storage-blob\n",
    "# !pip3 install opencv-python"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1668508110803
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data handlers\n",
    "\n",
    "This code contains the dataset splitter and loaders used during the process"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create data splits"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from azure.storage.blob import ContainerClient\n",
    "import re\n",
    "import io\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "WRK_PTH = os.getcwd()\n",
    "ENV_MODES = ['Easy']\n",
    "SIDES = ['Left', 'Right']\n",
    "NUM_VAL_TRAJECTS = 2\n",
    "SHUFFLE_TRAJ_DIRS = False\n",
    "SHUFFLE_IMAGES = False\n",
    "FRAME_LIMITS = [-1, 2]\n",
    "\n",
    "\n",
    "RUN = False # not ( os.path.exists(os.path.join(WRK_PTH, 'train_files.txt')) and os.path.exists(os.path.join(WRK_PTH, 'val_files.txt')) )\n",
    "if RUN:\n",
    "    # Dataset website: http://theairlab.org/tartanair-dataset/\n",
    "    account_url = 'https://tartanair.blob.core.windows.net/'\n",
    "    container_name = 'tartanair-release1'\n",
    "\n",
    "    container_client = ContainerClient(account_url=account_url, \n",
    "                                    container_name=container_name,\n",
    "                                    credential=None)\n",
    "\n",
    "\n",
    "def get_environment_list():\n",
    "    '''\n",
    "    List all the environments shown in the root directory\n",
    "    '''\n",
    "    env_gen = container_client.walk_blobs()\n",
    "    envlist = []\n",
    "    for env in env_gen:\n",
    "        envlist.append(env.name)\n",
    "    return envlist\n",
    "\n",
    "\n",
    "def get_trajectory_list(envname, easy_hard = 'Easy'):\n",
    "    '''\n",
    "    List all the trajectory folders, which is named as 'P0XX'\n",
    "    '''\n",
    "    assert(easy_hard=='Easy' or easy_hard=='Hard')\n",
    "    traj_gen = container_client.walk_blobs(name_starts_with=envname + '/' + easy_hard+'/')\n",
    "    trajlist = []\n",
    "    for traj in traj_gen:\n",
    "        trajname = traj.name\n",
    "        trajname_split = trajname.split('/')\n",
    "        trajname_split = [tt for tt in trajname_split if len(tt)>0]\n",
    "        if trajname_split[-1][0] == 'P':\n",
    "            trajlist.append(trajname)\n",
    "    return trajlist\n",
    "\n",
    "\n",
    "def _list_blobs_in_folder(folder_name):\n",
    "    \"\"\"\n",
    "    List all blobs in a virtual folder in an Azure blob container\n",
    "    \"\"\"\n",
    "    \n",
    "    files = []\n",
    "    generator = container_client.list_blobs(name_starts_with=folder_name)\n",
    "    for blob in generator:\n",
    "        files.append(blob.name)\n",
    "    return files\n",
    "\n",
    "\n",
    "def get_image_list(trajdir, left_right = 'left'):\n",
    "    assert(left_right == 'left' or left_right == 'right')\n",
    "    files = _list_blobs_in_folder(trajdir + '/image_' + left_right + '/')\n",
    "    files = [fn for fn in files if fn.endswith('.png')]\n",
    "    return files\n",
    "\n",
    "\n",
    "def append_paths_srings(env_name, env_mode, traj_dir_list, path_list):\n",
    "    for traj_dir in traj_dir_list:\n",
    "        traj_name = traj_dir.split('/')[-2]\n",
    "        img_pth_list = get_image_list(traj_dir)\n",
    "        img_pth_list = img_pth_list[abs(FRAME_LIMITS[0]):-FRAME_LIMITS[1]]\n",
    "\n",
    "        for img_pth in img_pth_list:\n",
    "            frame_id = re.findall('\\d\\d\\d\\d\\d\\d', img_pth)\n",
    "            \n",
    "            if len(frame_id) > 0:\n",
    "                frame_id = frame_id[0]\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            if 'right' in img_pth:\n",
    "                side = 'r'\n",
    "            elif 'left' in img_pth:\n",
    "                side = 'l'\n",
    "            else:\n",
    "                raise AttributeError\n",
    "\n",
    "            path_list += [\n",
    "                \"{} {} {}\".format(\"{}_{}_{}\".format(\n",
    "                    env_mode, env_name, traj_name), frame_id, side\n",
    "                )\n",
    "            ]\n",
    "\n",
    "\n",
    "def write_paths(file_path, paths):\n",
    "    with open(file_path, 'w') as txt_file:\n",
    "        for path in paths:\n",
    "            txt_file.write(\"{}\\n\".format(path))\n",
    "\n",
    "\n",
    "def create_blob_splits():\n",
    "    envlist = get_environment_list()\n",
    "    \n",
    "    train_paths = []\n",
    "    val_paths = []\n",
    "\n",
    "    for env_name in envlist:\n",
    "        for mode in ENV_MODES:\n",
    "            env_trajects = get_trajectory_list(env_name, easy_hard=mode)\n",
    "            \n",
    "            if SHUFFLE_TRAJ_DIRS:\n",
    "                rng = np.random.default_rng(42)\n",
    "                rng.shuffle(env_trajects)\n",
    "\n",
    "            train_trajects = env_trajects[:-NUM_VAL_TRAJECTS]\n",
    "            val_trajects = env_trajects[-NUM_VAL_TRAJECTS:]\n",
    "            \n",
    "            append_paths_srings(env_name[:-1], mode, train_trajects, train_paths)\n",
    "            append_paths_srings(env_name[:-1], mode, val_trajects, val_paths)\n",
    "            \n",
    "            # TODO: kiszedni\n",
    "            break\n",
    "        # TODO: kiszedni    \n",
    "        break\n",
    "\n",
    "    if SHUFFLE_IMAGES:\n",
    "        rng = np.random.default_rng(42)\n",
    "        rng.shuffle(train_paths)\n",
    "\n",
    "    write_paths(os.path.join(WRK_PTH, 'train_files.txt'), train_paths)\n",
    "    write_paths(os.path.join(WRK_PTH, 'val_files.txt'), val_paths)\n",
    "\n",
    "\n",
    "if RUN:\n",
    "    import time\n",
    "    start = time.time()\n",
    "    create_blob_splits()\n",
    "    print(time.time() - start)\n",
    "    print(time.time() - start)"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1668508111156
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data loaders"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Base class"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image  # using pillow-simd for increased speed\n",
    "import os\n",
    "import PIL.Image as pil\n",
    "from zipfile import ZipFile\n",
    "import re\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "from azure.storage.blob import ContainerClient\n",
    "\n",
    "\n",
    "def pil_zip_loader(path):\n",
    "    # open path as file to avoid ResourceWarning\n",
    "    # (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    pattern_dir = re.compile(\".*\\.zip\")\n",
    "    match_dir = re.match(pattern_dir, path)\n",
    "    img_path = path.split(\".zip\")[-1]\n",
    "    img_path = img_path.replace(\"\\\\\", \"/\")[1:]\n",
    "    with ZipFile(match_dir[0], 'r') as zip_dir:\n",
    "        with zip_dir.open(img_path.replace(\"\\\\\", \"/\")) as f:\n",
    "            with pil.open(f) as img:\n",
    "                return img.convert('RGB')\n",
    "\n",
    "\n",
    "# From https://github.com/nianticlabs/monodepth2\n",
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning\n",
    "    # (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert('RGB')\n",
    "\n",
    "\n",
    "class TartanAirDataset:\n",
    "    def __init__(self,\n",
    "         data_path,\n",
    "         filenames,\n",
    "         height,\n",
    "         width,\n",
    "         frame_idxs,\n",
    "         num_scales,\n",
    "         is_train=False,\n",
    "         img_ext='.png'):\n",
    "        \n",
    "        # Dataset website: http://theairlab.org/tartanair-dataset/\n",
    "        account_url = 'https://tartanair.blob.core.windows.net/'\n",
    "        container_name = 'tartanair-release1'\n",
    "        container_client = ContainerClient(account_url=account_url,\n",
    "                                        container_name=container_name,\n",
    "                                        credential=None)\n",
    "        self.container_client = container_client\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self.filenames = filenames\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.num_scales = num_scales\n",
    "        self.interp = Image.ANTIALIAS\n",
    "\n",
    "        self.frame_idxs = frame_idxs\n",
    "\n",
    "        self.is_train = is_train\n",
    "        self.img_ext = img_ext\n",
    "\n",
    "        self.loader = pil_loader\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "\n",
    "        # We need to specify augmentations differently in newer versions of torchvision.\n",
    "        # We first try the newer tuple version; if this fails we fall back to scalars\n",
    "        try:\n",
    "            self.brightness = (0.8, 1.2)\n",
    "            self.contrast = (0.8, 1.2)\n",
    "            self.saturation = (0.8, 1.2)\n",
    "            self.hue = (-0.1, 0.1)\n",
    "            transforms.ColorJitter.get_params(\n",
    "                self.brightness, self.contrast, self.saturation, self.hue)\n",
    "        except TypeError:\n",
    "            self.brightness = 0.2\n",
    "            self.contrast = 0.2\n",
    "            self.saturation = 0.2\n",
    "            self.hue = 0.1\n",
    "\n",
    "        self.resize = {}\n",
    "        for i in range(self.num_scales):\n",
    "            s = 2 ** i\n",
    "            self.resize[i] = transforms.Resize((self.height // s, self.width // s),\n",
    "                                               interpolation=self.interp)\n",
    "\n",
    "        self.load_depth = self.check_depth()\n",
    "        self.og_shape = (480, 640)  # (height, width)\n",
    "        self.loader = pil_zip_loader\n",
    "        # From https://arxiv.org/pdf/2011.00359.pdf\n",
    "        fx = 320 / self.og_shape[1]\n",
    "        fy = 320 / self.og_shape[0]\n",
    "        self.K = np.array([\n",
    "            [fx, 0., 0.5, 0.],\n",
    "            [0., fy, 0.5, 0.],\n",
    "            [0., 0., 1.0, 0.],\n",
    "            [0., 0., 0.0, 1.],\n",
    "        ], dtype=np.float32)\n",
    "        self.fov = 90\n",
    "        self.side_map = {\"l\": \"left\", \"r\": \"right\"}\n",
    "\n",
    "    def preprocess(self, inputs, color_aug):\n",
    "        \"\"\"Resize colour images to the required scales and augment if required\n",
    "\n",
    "        We create the color_aug object in advance and apply the same augmentation to all\n",
    "        images in this item. This ensures that all images input to the pose network receive the\n",
    "        same augmentation.\n",
    "        \"\"\"\n",
    "        for k in list(inputs):\n",
    "            frame = inputs[k]\n",
    "            if \"color\" in k:\n",
    "                n, im, i = k\n",
    "                for i in range(self.num_scales):\n",
    "                    inputs[(n, im, i)] = self.resize[i](inputs[(n, im, i - 1)])\n",
    "\n",
    "        for k in list(inputs):\n",
    "            f = inputs[k]\n",
    "            if \"color\" in k:\n",
    "                n, im, i = k\n",
    "                inputs[(n, im, i)] = self.to_tensor(f)\n",
    "                inputs[(n + \"_aug\", im, i)] = self.to_tensor(color_aug(f))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns a single training item from the dataset as a dictionary.\n",
    "\n",
    "        Values correspond to torch tensors.\n",
    "        Keys in the dictionary are either strings or tuples:\n",
    "\n",
    "            (\"color\", <frame_id>, <scale>)          for raw colour images,\n",
    "            (\"color_aug\", <frame_id>, <scale>)      for augmented colour images,\n",
    "            (\"K\", scale) or (\"inv_K\", scale)        for camera intrinsics,\n",
    "            \"stereo_T\"                              for camera extrinsics, and\n",
    "            \"depth_gt\"                              for ground truth depth maps.\n",
    "\n",
    "        <frame_id> is either:\n",
    "            an integer (e.g. 0, -1, or 1) representing the temporal step relative to 'index',\n",
    "        or\n",
    "            \"s\" for the opposite image in the stereo pair.\n",
    "\n",
    "        <scale> is an integer representing the scale of the image relative to the fullsize image:\n",
    "            -1      images at native resolution as loaded from disk\n",
    "            0       images resized to (self.width,      self.height     )\n",
    "            1       images resized to (self.width // 2, self.height // 2)\n",
    "            2       images resized to (self.width // 4, self.height // 4)\n",
    "            3       images resized to (self.width // 8, self.height // 8)\n",
    "        \"\"\"\n",
    "        inputs = {}\n",
    "\n",
    "        do_color_aug = self.is_train and random.random() > 0.5\n",
    "        do_flip = self.is_train and random.random() > 0.5\n",
    "\n",
    "        line = self.filenames[index].split()\n",
    "        folder = line[0]\n",
    "\n",
    "        if len(line) == 3:\n",
    "            frame_index = int(line[1])\n",
    "        else:\n",
    "            frame_index = 0\n",
    "\n",
    "        if len(line) == 3:\n",
    "            side = line[2]\n",
    "        else:\n",
    "            side = None\n",
    "\n",
    "        for i in self.frame_idxs:\n",
    "            if i == \"s\":\n",
    "                other_side = {\"r\": \"l\", \"l\": \"r\"}[side]\n",
    "                inputs[(\"color\", i, -1)] = self.get_color(folder, frame_index, other_side, do_flip)\n",
    "            else:\n",
    "                inputs[(\"color\", i, -1)] = self.get_color(folder, frame_index + i, side, do_flip)\n",
    "\n",
    "        # adjusting intrinsics to match each scale in the pyramid\n",
    "        for scale in range(self.num_scales):\n",
    "            K = self.K.copy()\n",
    "\n",
    "            K[0, :] *= self.width // (2 ** scale)\n",
    "            K[1, :] *= self.height // (2 ** scale)\n",
    "\n",
    "            inv_K = np.linalg.pinv(K)\n",
    "\n",
    "            inputs[(\"K\", scale)] = torch.from_numpy(K)\n",
    "            inputs[(\"inv_K\", scale)] = torch.from_numpy(inv_K)\n",
    "\n",
    "        if do_color_aug:\n",
    "            color_aug = transforms.ColorJitter(\n",
    "                self.brightness, self.contrast, self.saturation, self.hue)\n",
    "        else:\n",
    "            color_aug = (lambda x: x)\n",
    "\n",
    "        self.preprocess(inputs, color_aug)\n",
    "\n",
    "        for i in self.frame_idxs:\n",
    "            del inputs[(\"color\", i, -1)]\n",
    "            del inputs[(\"color_aug\", i, -1)]\n",
    "\n",
    "        if self.load_depth:\n",
    "            depth_gt = self.get_depth(folder, frame_index, side, do_flip)\n",
    "            inputs[\"depth_gt\"] = np.expand_dims(depth_gt, 0)\n",
    "            inputs[\"depth_gt\"] = torch.from_numpy(inputs[\"depth_gt\"].astype(np.float32))\n",
    "\n",
    "        if \"s\" in self.frame_idxs:\n",
    "            stereo_T = np.eye(4, dtype=np.float32)\n",
    "            baseline_sign = -1 if do_flip else 1\n",
    "            side_sign = -1 if side == \"l\" else 1\n",
    "            stereo_T[0, 3] = side_sign * baseline_sign * 0.1\n",
    "\n",
    "            inputs[\"stereo_T\"] = torch.from_numpy(stereo_T)\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def get_color(self, folder, frame_index, side, do_flip):\n",
    "        image_file = self.get_image_path(folder, frame_index, side)\n",
    "        bc = self.container_client.get_blob_client(blob=image_file)\n",
    "        data = bc.download_blob()\n",
    "        ee = io.BytesIO(data.content_as_bytes())\n",
    "        img = cv2.imdecode(np.asarray(bytearray(ee.read()),dtype=np.uint8), cv2.IMREAD_COLOR)\n",
    "        color = img[:, :, [2, 1, 0]] # BGR2RGB\n",
    "        color = pil.fromarray(color, \"RGB\")\n",
    "\n",
    "        if do_flip:\n",
    "            color = color.transpose(pil.FLIP_LEFT_RIGHT)\n",
    "\n",
    "        return color\n",
    "\n",
    "    def get_image_path(self, folder, frame_index, side):\n",
    "        folder_parts = folder.split('_')\n",
    "        if len(folder_parts) == 3:\n",
    "            mode, env, traj = folder_parts\n",
    "        elif len(folder_parts) == 4:\n",
    "            mode = folder_parts[0]\n",
    "            env = f'{folder_parts[1]}_{folder_parts[2]}'\n",
    "            traj = folder_parts[3]\n",
    "        else:\n",
    "            print('Error in getting image path!')\n",
    "            raise ValueError\n",
    "        side = self.side_map[side]\n",
    "        img_folder = \"image_{}\".format(side)\n",
    "        f_str = \"{:06d}_{}{}\".format(frame_index, side, self.img_ext)\n",
    "        image_path = os.path.join(env, mode, traj, img_folder, f_str)\n",
    "        return image_path\n",
    "\n",
    "    def check_depth(self):\n",
    "        return None\n",
    "\n",
    "    def get_depth(self, folder, frame_index, side, do_flip):\n",
    "        raise NotImplementedError"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1668508111824
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Models and utils"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Util functions"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transformations"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "def disp2depth(disp, min_depth, max_depth):\n",
    "    \"\"\"Convert network's sigmoid output into depth prediction\n",
    "    The formula for this conversion is given in the 'additional considerations'\n",
    "    section of the paper.\n",
    "    from https://github.com/nianticlabs/monodepth2/blob/master/layers.py\n",
    "    \"\"\"\n",
    "    min_disp = 1 / max_depth\n",
    "    max_disp = 1 / min_depth\n",
    "    scaled_disp = min_disp + (max_disp - min_disp) * disp\n",
    "    depth = 1 / scaled_disp\n",
    "    return scaled_disp, depth\n",
    "\n",
    "\n",
    "def get_translation_matrix(translation_vector):\n",
    "    \"\"\"Convert a translation vector into a 4x4 transformation matrix\n",
    "    from https://github.com/nianticlabs/monodepth2/blob/master/layers.py\n",
    "    \"\"\"\n",
    "    T = torch.zeros(translation_vector.shape[0], 4, 4).to(device=translation_vector.device)\n",
    "\n",
    "    t = translation_vector.contiguous().view(-1, 3, 1)\n",
    "\n",
    "    T[:, 0, 0] = 1\n",
    "    T[:, 1, 1] = 1\n",
    "    T[:, 2, 2] = 1\n",
    "    T[:, 3, 3] = 1\n",
    "    T[:, :3, 3, None] = t\n",
    "\n",
    "    return T\n",
    "\n",
    "\n",
    "def rot_from_axisangle(vec):\n",
    "    \"\"\"Convert an axisangle rotation into a 4x4 transformation matrix\n",
    "    (adapted from https://github.com/Wallacoloo/printipi)\n",
    "    Input 'vec' has to be Bx1x3\n",
    "    from https://github.com/nianticlabs/monodepth2/blob/master/layers.py\n",
    "    \"\"\"\n",
    "    angle = torch.norm(vec, 2, 2, True)\n",
    "    axis = vec / (angle + 1e-7)\n",
    "\n",
    "    ca = torch.cos(angle)\n",
    "    sa = torch.sin(angle)\n",
    "    C = 1 - ca\n",
    "\n",
    "    x = axis[..., 0].unsqueeze(1)\n",
    "    y = axis[..., 1].unsqueeze(1)\n",
    "    z = axis[..., 2].unsqueeze(1)\n",
    "\n",
    "    xs = x * sa\n",
    "    ys = y * sa\n",
    "    zs = z * sa\n",
    "    xC = x * C\n",
    "    yC = y * C\n",
    "    zC = z * C\n",
    "    xyC = x * yC\n",
    "    yzC = y * zC\n",
    "    zxC = z * xC\n",
    "\n",
    "    rot = torch.zeros((vec.shape[0], 4, 4)).to(device=vec.device)\n",
    "\n",
    "    rot[:, 0, 0] = torch.squeeze(x * xC + ca)\n",
    "    rot[:, 0, 1] = torch.squeeze(xyC - zs)\n",
    "    rot[:, 0, 2] = torch.squeeze(zxC + ys)\n",
    "    rot[:, 1, 0] = torch.squeeze(xyC + zs)\n",
    "    rot[:, 1, 1] = torch.squeeze(y * yC + ca)\n",
    "    rot[:, 1, 2] = torch.squeeze(yzC - xs)\n",
    "    rot[:, 2, 0] = torch.squeeze(zxC - ys)\n",
    "    rot[:, 2, 1] = torch.squeeze(yzC + xs)\n",
    "    rot[:, 2, 2] = torch.squeeze(z * zC + ca)\n",
    "    rot[:, 3, 3] = 1\n",
    "\n",
    "    return rot\n",
    "\n",
    "\n",
    "def transformation_from_parameters(axisangle, translation, invert=False):\n",
    "    \"\"\"Convert the network's (axisangle, translation) output into a 4x4 matrix\n",
    "    from https://github.com/nianticlabs/monodepth2/blob/master/layers.py\n",
    "    \"\"\"\n",
    "    R = rot_from_axisangle(axisangle)\n",
    "    t = translation.clone()\n",
    "\n",
    "    if invert:\n",
    "        R = R.transpose(1, 2)\n",
    "        t *= -1\n",
    "\n",
    "    T = get_translation_matrix(t)\n",
    "\n",
    "    if invert:\n",
    "        M = torch.matmul(R, T)\n",
    "    else:\n",
    "        M = torch.matmul(T, R)\n",
    "\n",
    "    return M\n",
    "\n",
    "\n",
    "class BackprojectDepth(nn.Module):\n",
    "    \"\"\"Layer to transform a depth image into a point cloud\n",
    "    from https://github.com/nianticlabs/monodepth2/blob/master/layers.py\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, height, width):\n",
    "        super(BackprojectDepth, self).__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "        meshgrid = np.meshgrid(range(self.width), range(self.height), indexing='xy')\n",
    "        self.id_coords = np.stack(meshgrid, axis=0).astype(np.float32)\n",
    "        self.id_coords = nn.Parameter(torch.from_numpy(self.id_coords),\n",
    "                                      requires_grad=False)\n",
    "\n",
    "        self.ones = nn.Parameter(torch.ones(self.batch_size, 1, self.height * self.width),\n",
    "                                 requires_grad=False)\n",
    "\n",
    "        self.pix_coords = torch.unsqueeze(torch.stack(\n",
    "            [self.id_coords[0].view(-1), self.id_coords[1].view(-1)], 0), 0)\n",
    "        self.pix_coords = self.pix_coords.repeat(batch_size, 1, 1)\n",
    "        self.pix_coords = nn.Parameter(torch.cat([self.pix_coords, self.ones], 1),\n",
    "                                       requires_grad=False)\n",
    "\n",
    "    def forward(self, depth, inv_K):\n",
    "        cam_points = torch.matmul(inv_K[:, :3, :3], self.pix_coords)\n",
    "        cam_points = depth.view(self.batch_size, 1, -1) * cam_points\n",
    "        cam_points = torch.cat([cam_points, self.ones], 1)\n",
    "\n",
    "        return cam_points\n",
    "\n",
    "\n",
    "class Project3D(nn.Module):\n",
    "    \"\"\"Layer which projects 3D points into a camera with intrinsics K and at position T\n",
    "    from https://github.com/nianticlabs/monodepth2/blob/master/layers.py\n",
    "    \"\"\"\n",
    "    def __init__(self, height, width, eps=1e-7):\n",
    "        super(Project3D, self).__init__()\n",
    "\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, points, K, T):\n",
    "        P = torch.matmul(K, T)[:, :3, :]\n",
    "\n",
    "        cam_points = torch.matmul(P, points)\n",
    "\n",
    "        pix_coords = cam_points[:, :2, :] / (cam_points[:, 2, :].unsqueeze(1) + self.eps)\n",
    "        pix_coords = pix_coords.view(-1, 2, self.height, self.width)\n",
    "        pix_coords = pix_coords.permute(0, 2, 3, 1)\n",
    "        pix_coords[..., 0] /= self.width - 1\n",
    "        pix_coords[..., 1] /= self.height - 1\n",
    "        pix_coords = (pix_coords - 0.5) * 2\n",
    "        return pix_coords\n"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1668508112035
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Losses"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_smooth_loss(disp: torch.Tensor, img: torch.Tensor):\n",
    "    \"\"\"Computes the smoothness loss for a disparity image\n",
    "    The color image is used for edge-aware smoothness\n",
    "    from https://github.com/nianticlabs/monodepth2/blob/master/layers.py\n",
    "    \"\"\"\n",
    "    grad_disp_x = torch.abs(disp[:, :, :, :-1] - disp[:, :, :, 1:])\n",
    "    grad_disp_y = torch.abs(disp[:, :, :-1, :] - disp[:, :, 1:, :])\n",
    "\n",
    "    grad_img_x = torch.mean(torch.abs(img[:, :, :, :-1] - img[:, :, :, 1:]), 1, keepdim=True)\n",
    "    grad_img_y = torch.mean(torch.abs(img[:, :, :-1, :] - img[:, :, 1:, :]), 1, keepdim=True)\n",
    "\n",
    "    grad_disp_x *= torch.exp(-grad_img_x)\n",
    "    grad_disp_y *= torch.exp(-grad_img_y)\n",
    "\n",
    "    return grad_disp_x.mean() + grad_disp_y.mean()\n",
    "\n",
    "\n",
    "class SSIM(nn.Module):\n",
    "    \"\"\"Layer to compute the SSIM loss between a pair of images\n",
    "    from https://github.com/nianticlabs/monodepth2/blob/master/layers.py\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SSIM, self).__init__()\n",
    "        self.mu_x_pool   = nn.AvgPool2d(3, 1)\n",
    "        self.mu_y_pool   = nn.AvgPool2d(3, 1)\n",
    "        self.sig_x_pool  = nn.AvgPool2d(3, 1)\n",
    "        self.sig_y_pool  = nn.AvgPool2d(3, 1)\n",
    "        self.sig_xy_pool = nn.AvgPool2d(3, 1)\n",
    "\n",
    "        self.refl = nn.ReflectionPad2d(1)\n",
    "\n",
    "        self.C1 = 0.01 ** 2\n",
    "        self.C2 = 0.03 ** 2\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.refl(x)\n",
    "        y = self.refl(y)\n",
    "\n",
    "        mu_x = self.mu_x_pool(x)\n",
    "        mu_y = self.mu_y_pool(y)\n",
    "\n",
    "        sigma_x  = self.sig_x_pool(x ** 2) - mu_x ** 2\n",
    "        sigma_y  = self.sig_y_pool(y ** 2) - mu_y ** 2\n",
    "        sigma_xy = self.sig_xy_pool(x * y) - mu_x * mu_y\n",
    "\n",
    "        SSIM_n = (2 * mu_x * mu_y + self.C1) * (2 * sigma_xy + self.C2)\n",
    "        SSIM_d = (mu_x ** 2 + mu_y ** 2 + self.C1) * (sigma_x + sigma_y + self.C2)\n",
    "\n",
    "        return torch.clamp((1 - SSIM_n / SSIM_d) / 2, 0, 1)\n",
    "\n",
    "\n",
    "class MVS3DLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-view 3D structure consistency loss\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size: int, height: int, width: int, reduction: str = 'mean'):\n",
    "        super(MVS3DLoss, self).__init__()\n",
    "        self.back_project = BackprojectDepth(batch_size, height, width)\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            depth_src: torch.Tensor,\n",
    "            depth_tgt: torch.Tensor,\n",
    "            pose: torch.Tensor,\n",
    "            inv_intrinsics: torch.Tensor\n",
    "    ):\n",
    "        cloud_src = self.back_project(depth_src, inv_intrinsics)\n",
    "        cloud_tgt = self.back_project(depth_tgt, inv_intrinsics)\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return (cloud_tgt - pose @ cloud_src).abs().mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return (cloud_tgt - pose @ cloud_src).abs().sum()\n",
    "        elif self.reduction == 'none':\n",
    "            return (cloud_tgt - pose @ cloud_src).abs()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "class EpipolarLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Epipolar geometry for loss calculation\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size: int, height: int, width: int, pix_group_size: int = 128, reduction: str = 'mean'):\n",
    "        super(EpipolarLoss, self).__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.pix_group_size = pix_group_size\n",
    "        self.reduction = reduction\n",
    "\n",
    "        meshgrid = np.meshgrid(range(self.width), range(self.height), indexing='xy')\n",
    "        self.id_coords = np.stack(meshgrid, axis=0).astype(np.float32)\n",
    "        self.id_coords = nn.Parameter(torch.from_numpy(self.id_coords),\n",
    "                                      requires_grad=False)\n",
    "\n",
    "        self.ones = nn.Parameter(torch.ones(self.batch_size, 1, self.height * self.width),\n",
    "                                 requires_grad=False)\n",
    "\n",
    "        self.pix_coords = torch.unsqueeze(torch.stack(\n",
    "            [self.id_coords[0].view(-1), self.id_coords[1].view(-1)], 0), 0)\n",
    "        self.pix_coords = self.pix_coords.repeat(batch_size, 1, 1)\n",
    "        self.pix_coords = nn.Parameter(torch.cat([self.pix_coords, self.ones], 1),\n",
    "                                       requires_grad=False)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            flow: torch.Tensor,\n",
    "            pose: torch.Tensor,\n",
    "            inv_intrinsics: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        rotation = pose[:, :3, :3]\n",
    "        translation = pose[:, :3, -1].unsqueeze(1)\n",
    "\n",
    "        # Translation skew-matrix\n",
    "        t_skew = torch.zeros_like(translation)\n",
    "        t_skew = t_skew.repeat(1, translation.shape[-1], 1)\n",
    "        t_skew[:, 0, 1] = -translation[..., 2]\n",
    "        t_skew[:, 0, 2] = translation[..., 1]\n",
    "        t_skew[:, 1, 2] = -translation[..., 0]\n",
    "        t_skew = t_skew - t_skew.transpose(1, 2)\n",
    "\n",
    "        # Move pixels with flow\n",
    "        flattened_flow = flow.view(flow.shape[0], flow.shape[1], -1)\n",
    "        pix_flow = torch.ones_like(self.pix_coords)\n",
    "        pix_flow[:, :2] = self.pix_coords[:, :2] + flattened_flow\n",
    "\n",
    "        # Epipolar geometry using the predicted flow as target image\n",
    "        if self.pix_group_size > 1:\n",
    "            pix_losses = None\n",
    "            for idx in range(0, self.pix_coords.shape[-1], self.pix_group_size):\n",
    "                pix_coords = self.pix_coords[..., idx:idx+self.pix_group_size]\n",
    "                poseT_mm_invT = pix_coords.transpose(1, 2) @ inv_intrinsics[:, :3, :3].transpose(1, 2)\n",
    "                rot_tskew_inv = rotation @ t_skew @ inv_intrinsics[:, :3, :3]\n",
    "                if pix_losses is not None:\n",
    "                    pix_losses = pix_losses + \\\n",
    "                                 (poseT_mm_invT @ rot_tskew_inv @ pix_flow[..., idx:idx+self.pix_group_size]\n",
    "                                  ).sum()\n",
    "                else:\n",
    "                    pix_losses = (poseT_mm_invT @ rot_tskew_inv @ pix_flow[..., idx:idx+self.pix_group_size]\n",
    "                                  ).sum()\n",
    "        else:\n",
    "            pix_losses = self.pix_coords.transpose(1, 2) @ inv_intrinsics[:, :3, :3].transpose(1, 2) \\\n",
    "                         @ rotation @ t_skew @ inv_intrinsics[:, :3, :3] @ pix_flow\n",
    "\n",
    "        if self.reduction == 'sum':\n",
    "            if self.pix_group_size > 1:\n",
    "                return pix_losses\n",
    "            else:\n",
    "                return pix_losses.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            if self.pix_group_size > 1:\n",
    "                return pix_losses / self.pix_coords.shape[-1]\n",
    "            else:\n",
    "                return pix_losses.mean()\n",
    "        elif self.reduction == 'none':\n",
    "            return pix_losses\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "class AdaptivePhotometricLoss(nn.Module):\n",
    "    def __init__(self, batch_size: int, height: int, width: int, r: float = 0.85, reduction: str = 'mean'):\n",
    "        super(AdaptivePhotometricLoss, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.reduction = reduction\n",
    "\n",
    "        self.project = Project3D(height, width)\n",
    "        self.back_project = BackprojectDepth(batch_size, height, width)\n",
    "        self.ssim = SSIM()\n",
    "        self.r = r\n",
    "\n",
    "        meshgrid = np.meshgrid(range(self.width), range(self.height), indexing='xy')\n",
    "        self.id_coords = np.stack(meshgrid, axis=0).astype(np.float32)\n",
    "        self.id_coords = nn.Parameter(torch.from_numpy(self.id_coords),\n",
    "                                      requires_grad=False)\n",
    "\n",
    "        self.ones = nn.Parameter(torch.ones(self.batch_size, 1, self.height * self.width),\n",
    "                                 requires_grad=False)\n",
    "\n",
    "        self.pix_coords = torch.unsqueeze(torch.stack(\n",
    "            [self.id_coords[0].view(-1), self.id_coords[1].view(-1)], 0), 0)\n",
    "        self.pix_coords = self.pix_coords.repeat(batch_size, 1, 1)\n",
    "        self.pix_coords = nn.Parameter(torch.cat([self.pix_coords, self.ones], 1),\n",
    "                                       requires_grad=False)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            img_src: torch.Tensor,\n",
    "            img_tgt: torch.Tensor,\n",
    "            depth: torch.Tensor,\n",
    "            flow: torch.Tensor,\n",
    "            pose: torch.Tensor,\n",
    "            inv_intrinsics: torch.Tensor\n",
    "    ):\n",
    "        # 3D warp target image\n",
    "        cam_coord_src = self.back_project(depth, inv_intrinsics)\n",
    "        pix_coords_tgt = self.project(cam_coord_src, inv_intrinsics, pose)\n",
    "        warped_tgt_3d = F.grid_sample(img_src, pix_coords_tgt,\n",
    "                                      padding_mode='border', align_corners=True)\n",
    "\n",
    "        # Flow warp target image\n",
    "        pix_coords = self.pix_coords[:, :2].view(self.batch_size, 2, self.height, self.width).contiguous()\n",
    "        pix_flow = pix_coords + flow\n",
    "        warped_tgt_flow = F.grid_sample(img_src, pix_flow.permute(0, 2, 3, 1).contiguous(),\n",
    "                                        padding_mode='border', align_corners=True)\n",
    "\n",
    "        # Pixel-wise minimum of SSIM maps\n",
    "        ssim_3d = self.ssim(img_tgt, warped_tgt_3d).mean(1, keepdim=True)\n",
    "        ssim_flow = self.ssim(img_tgt, warped_tgt_flow).mean(1, keepdim=True)\n",
    "\n",
    "        l1_3d = (img_tgt - warped_tgt_3d).abs().mean(1, keepdim=True)\n",
    "        l1_flow = (img_tgt - warped_tgt_flow).abs().mean(1, keepdim=True)\n",
    "\n",
    "        s_3d = self.r * (1 - ssim_3d) / 2 + (1 - self.r) * l1_3d\n",
    "        s_flow = self.r * (1 - ssim_flow) / 2 + (1 - self.r) * l1_flow\n",
    "\n",
    "        apc_loss = torch.stack([s_3d, s_flow], dim=1)\n",
    "        apc_loss = apc_loss.min(dim=1)[0]\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return apc_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return apc_loss.sum()\n",
    "        elif self.reduction == 'none':\n",
    "            return apc_loss\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "class FwdBwdFlowConsistency(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            batch_size: int,\n",
    "            height: int,\n",
    "            width: int,\n",
    "            alpha: float = 3.,\n",
    "            beta: float = 0.05,\n",
    "            scale: int = 0,\n",
    "            reduction: str = 'mean'\n",
    "    ):\n",
    "        super(FwdBwdFlowConsistency, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.scale = scale\n",
    "        self.reduction = reduction\n",
    "\n",
    "        meshgrid = np.meshgrid(range(self.width), range(self.height), indexing='xy')\n",
    "        self.id_coords = np.stack(meshgrid, axis=0).astype(np.float32)\n",
    "        self.id_coords = nn.Parameter(torch.from_numpy(self.id_coords),\n",
    "                                      requires_grad=False)\n",
    "\n",
    "        self.ones = nn.Parameter(torch.ones(self.batch_size, 1, self.height * self.width),\n",
    "                                 requires_grad=False)\n",
    "\n",
    "        self.pix_coords = torch.unsqueeze(torch.stack(\n",
    "            [self.id_coords[0].view(-1), self.id_coords[1].view(-1)], 0), 0)\n",
    "        self.pix_coords = self.pix_coords.repeat(batch_size, 1, 1)\n",
    "        self.pix_coords = nn.Parameter(torch.cat([self.pix_coords, self.ones], 1),\n",
    "                                       requires_grad=False)\n",
    "\n",
    "    def forward(self, flow_fwd: torch.Tensor, flow_bwd: torch.Tensor):\n",
    "        # Warp\n",
    "        bwd2fwd = F.grid_sample(flow_bwd, flow_fwd, padding_mode='border', align_corners=True)\n",
    "        fwd2bwd = F.grid_sample(flow_fwd, flow_bwd, padding_mode='border', align_corners=True)\n",
    "\n",
    "        # Consistency error\n",
    "        diff_fwd = (bwd2fwd + flow_fwd).abs()\n",
    "        diff_bwd = (fwd2bwd + flow_bwd).abs()\n",
    "\n",
    "        # Condition\n",
    "        bound_fwd = self.beta * (2 ** self.scale) * flow_fwd.norm(p=2, dim=1, keepdim=True)\n",
    "        with torch.no_grad:\n",
    "            bound_fwd = bound_fwd.clamp_min(self.alpha)\n",
    "\n",
    "        bound_bwd = self.beta * (2 ** self.scale) * flow_bwd.norm(p=2, dim=1, keepdim=True)\n",
    "        with torch.no_grad:\n",
    "            bound_bwd = bound_bwd.clamp_min(self.alpha)\n",
    "\n",
    "        # Mask\n",
    "        noc_mask_src = ((2 ** self.scale) * diff_bwd.norm(p=2, dim=1, keepdim=True) < bound_bwd)\n",
    "        noc_mask_tgt = ((2 ** self.scale) * diff_fwd.norm(p=2, dim=1, keepdim=True) < bound_fwd)\n",
    "\n",
    "        # Consistency loss\n",
    "        loss_fwd = (diff_fwd.mean(dim=1, keepdim=True) * noc_mask_tgt).sum() / noc_mask_tgt.sum()\n",
    "        loss_bwd = (diff_bwd.mean(dim=1, keepdim=True) * noc_mask_src).sum() / noc_mask_src.sum()\n",
    "        consistency_loss = (loss_fwd + loss_bwd) / 2\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return consistency_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return consistency_loss.sum()\n",
    "        elif self.reduction == 'none':\n",
    "            return consistency_loss\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "def run_once(f):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        if not wrapper.has_run:\n",
    "            wrapper.has_run = True\n",
    "            return f(*args, **kwargs)\n",
    "    wrapper.has_run = False\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "class GLNetLoss(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            img_size: tuple,\n",
    "            scales: int,\n",
    "            mvs_weight: float,\n",
    "            epi_weight: float,\n",
    "            apc_weight: float,\n",
    "            disp_smooth: float,\n",
    "            flow_smooth: float,\n",
    "            flow_cons_params: tuple = None,\n",
    "            flow_cons_weight: float = 0.,\n",
    "            ssim_r: float = 0.85,\n",
    "            reduction: str = 'mean'\n",
    "    ):\n",
    "        super(GLNetLoss, self).__init__()\n",
    "        self.height = img_size[0]\n",
    "        self.width = img_size[1]\n",
    "        self.scales = scales\n",
    "        self.mvs_weight = mvs_weight\n",
    "        self.epi_weight = epi_weight\n",
    "        self.apc_weight = apc_weight\n",
    "        self.disp_smooth = disp_smooth\n",
    "        self.flow_smooth = flow_smooth\n",
    "        self.flow_cons_params = flow_cons_params\n",
    "        self.flow_cons_weight = flow_cons_weight\n",
    "        self.ssim_r = ssim_r\n",
    "        self.reduction = reduction\n",
    "\n",
    "        self.pix_coords_pyramid = {}\n",
    "        for scale in range(scales):\n",
    "            self.pix_coords_pyramid[scale] = \\\n",
    "                self.__generate_pix_coords(self.height // (2 ** scale), self.width // (2 ** scale))\n",
    "\n",
    "        self.ssim_f_3d = SSIM()\n",
    "        self.ssim_f_flow = SSIM()\n",
    "\n",
    "    def __generate_pix_coords(self, height: int, width: int):\n",
    "        meshgrid = np.meshgrid(range(width), range(height), indexing='xy')\n",
    "        id_coords = np.stack(meshgrid, axis=0).astype(np.float32)\n",
    "        id_coords = torch.from_numpy(id_coords)\n",
    "        ones = torch.ones(1, 1, height * width)\n",
    "        pix_coords = torch.unsqueeze(\n",
    "            torch.stack([id_coords[0].view(-1), id_coords[1].view(-1)], 0)\n",
    "            , 0\n",
    "        )\n",
    "        pix_coords = torch.cat([pix_coords, ones], 1)\n",
    "        return pix_coords\n",
    "\n",
    "    @run_once\n",
    "    def __set_batch_size(self, batch_size: int, ref_tensor: torch.Tensor):\n",
    "        self.batch_size = batch_size\n",
    "        for i in range(len(self.pix_coords_pyramid)):\n",
    "            self.pix_coords_pyramid[i] = self.pix_coords_pyramid[i].repeat(batch_size, 1, 1).to(ref_tensor)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            inputs: dict,\n",
    "            depths: dict,\n",
    "            poses: dict,\n",
    "            flows_fwd: dict,\n",
    "            scales: int,\n",
    "            disps: dict = None,\n",
    "            flows_bwd: dict = None\n",
    "    ):\n",
    "        pose_frames = list(poses.keys())\n",
    "        flow_frames = list(flows_fwd.keys())\n",
    "\n",
    "        calc_disp_smooth = (disps is not None and self.disp_smooth != 0.)\n",
    "        calc_flow_smooth = self.flow_smooth != 0.\n",
    "        calc_flow_consistency = (flows_bwd is not None and self.flow_cons_weight != 0.)\n",
    "\n",
    "        mvs_loss = 0.\n",
    "        epipolar_loss = 0.\n",
    "        apc_loss = 0.\n",
    "        disp_smooth_loss = 0.\n",
    "        flow_smooth_loss = 0.\n",
    "        flow_consistency_loss = 0.\n",
    "\n",
    "        for scale in range(scales):\n",
    "            for frame_group in list(set(pose_frames) & set(flow_frames)):\n",
    "                pose_params = poses[frame_group]\n",
    "                flow_fwd = flows_fwd[frame_group][('flow', scale)]\n",
    "                depth_src = depths[frame_group[0]][('depth', scale)]\n",
    "                depth_tgt = depths[frame_group[-1]][('depth', scale)]\n",
    "                inv_intrinsics = inputs[('inv_K', scale)]\n",
    "                intrinsics = inputs[('K', scale)]\n",
    "                image_src = inputs[('color', frame_group[0], scale)]\n",
    "                image_tgt = inputs[('color', frame_group[-1], scale)]\n",
    "\n",
    "                pose = transformation_from_parameters(pose_params['axisangle'], pose_params['translation'])\n",
    "\n",
    "                # Multi-view 3D structure loss\n",
    "                mvs_loss += self.mvs3d_loss(depth_src, depth_tgt, pose, inv_intrinsics, scale)\n",
    "\n",
    "                # Epipolar loss\n",
    "                epipolar_loss += self.epipolar_loss(flow_fwd, pose, inv_intrinsics, scale, pix_group_size=128)\n",
    "\n",
    "                # Adaptive Photometric Loss\n",
    "                apc_loss += self.adaptive_photometric_loss(image_src, image_tgt, depth_src, flow_fwd,\n",
    "                                                           pose, intrinsics, inv_intrinsics, scale)\n",
    "\n",
    "            if calc_disp_smooth:\n",
    "                # Disparity smoothness loss\n",
    "                for frame_id, disp_i in disps.items():\n",
    "                    disp_e = disp_i[('disp', scale)]\n",
    "                    mean_disp = disp_e.mean(2, True).mean(3, True)\n",
    "                    norm_disp = disp_e / (mean_disp + 1e-7)\n",
    "                    disp_smooth_loss += \\\n",
    "                        (self.disp_smooth / (2 ** scale) *\n",
    "                         get_smooth_loss(norm_disp, inputs[('color', frame_id, scale)]))\n",
    "\n",
    "            if calc_flow_smooth:\n",
    "                # Flow smoothness loss\n",
    "                div = (2 ** (scale + 1)) if flows_bwd is not None else (2 ** scale)\n",
    "                for frame_group, flow_fwd_i in flows_fwd.items():\n",
    "                    for chan in range(2):\n",
    "                        flow_smooth_loss += (self.flow_smooth / div *\n",
    "                                             get_smooth_loss(flow_fwd_i[('flow', scale)][:, chan].unsqueeze(1),\n",
    "                                                             inputs[('color', frame_group[-1], scale)]))\n",
    "                        if flows_bwd is not None:\n",
    "                            flow_bwd_i = flows_bwd[frame_group]\n",
    "                            flow_smooth_loss += (self.flow_smooth / div *\n",
    "                                                 get_smooth_loss(flow_bwd_i[('flow', scale)][:, chan].unsqueeze(1),\n",
    "                                                                 inputs[('color', frame_group[0], scale)]))\n",
    "\n",
    "            if calc_flow_consistency:\n",
    "                # Forward-backward flow consistency\n",
    "                for f_idx, flow_fwd_i in flows_fwd.items():\n",
    "                    flow_bwd_i = flows_bwd[f_idx]\n",
    "                    flow_consistency_loss += self.fwd_bwd_flow_consistency(flow_fwd_i[('flow', scale)],\n",
    "                                                                           flow_bwd_i[('flow', scale)],\n",
    "                                                                           scale)\n",
    "\n",
    "        total_loss = self.mvs_weight * mvs_loss + self.epi_weight * epipolar_loss + self.apc_weight * apc_loss\n",
    "        if calc_disp_smooth:\n",
    "            total_loss += disp_smooth_loss\n",
    "        if calc_flow_smooth:\n",
    "            total_loss += flow_smooth_loss\n",
    "        if calc_flow_consistency:\n",
    "            total_loss += (self.flow_cons_weight * flow_consistency_loss)\n",
    "        total_loss /= self.scales\n",
    "\n",
    "        loss_parts = {\n",
    "            'mvs': mvs_loss / self.scales,\n",
    "            'epi': epipolar_loss / self.scales,\n",
    "            'apc': apc_loss / self.scales,\n",
    "            'ds': disp_smooth_loss / self.scales,\n",
    "            'fs': flow_smooth_loss / self.scales,\n",
    "            'fc': flow_consistency_loss / self.scales\n",
    "        }\n",
    "        return total_loss, loss_parts\n",
    "\n",
    "    def back_project(self, depth: torch.Tensor, inv_intrinsics: torch.Tensor, scale: int) -> torch.Tensor:\n",
    "        self.__set_batch_size(depth.shape[0], depth)\n",
    "\n",
    "        cam_points = torch.matmul(inv_intrinsics[:, :3, :3], self.pix_coords_pyramid[scale])\n",
    "        cam_points = depth.view(self.batch_size, 1, -1) * cam_points\n",
    "        cam_points = torch.cat([cam_points, self.pix_coords_pyramid[scale][:, -1].unsqueeze(1)], 1)\n",
    "\n",
    "        return cam_points\n",
    "\n",
    "    def project_3d(self, cloud: torch.Tensor, intrinsics: torch.Tensor, pose: torch.Tensor, scale: int) -> torch.Tensor:\n",
    "        self.__set_batch_size(cloud.shape[0], cloud)\n",
    "\n",
    "        P = torch.matmul(intrinsics, pose)[:, :3, :]\n",
    "        cam_points = torch.matmul(P, cloud)\n",
    "        pix_coords = cam_points[:, :2, :] / (cam_points[:, 2, :].unsqueeze(1) + 1e-6)\n",
    "        pix_coords = pix_coords.view(-1, 2, self.height // (2 ** scale), self.width // (2 ** scale))\n",
    "        pix_coords = pix_coords.permute(0, 2, 3, 1)\n",
    "        pix_coords[..., 0] /= self.width // (2 ** scale) - 1\n",
    "        pix_coords[..., 1] /= self.height // (2 ** scale) - 1\n",
    "        pix_coords = (pix_coords - 0.5) * 2\n",
    "        return pix_coords\n",
    "\n",
    "    def mvs3d_loss(\n",
    "            self,\n",
    "            depth_src: torch.Tensor,\n",
    "            depth_tgt: torch.Tensor,\n",
    "            pose: torch.Tensor,\n",
    "            inv_intrinsics: torch.Tensor,\n",
    "            scale: int\n",
    "    ):\n",
    "        self.__set_batch_size(depth_src.shape[0], depth_src)\n",
    "        cloud_src = self.back_project(depth_src, inv_intrinsics, scale)\n",
    "        cloud_tgt = self.back_project(depth_tgt, inv_intrinsics, scale)\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return (cloud_tgt - pose @ cloud_src).abs().mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return (cloud_tgt - pose @ cloud_src).abs().sum()\n",
    "        elif self.reduction == 'none':\n",
    "            return (cloud_tgt - pose @ cloud_src).abs()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def epipolar_loss(\n",
    "            self,\n",
    "            flow: torch.Tensor,\n",
    "            pose: torch.Tensor,\n",
    "            inv_intrinsics: torch.Tensor,\n",
    "            scale: int,\n",
    "            pix_group_size: int = 1,\n",
    "    ):\n",
    "        self.__set_batch_size(pose.shape[0], pose)\n",
    "        pix_coords = self.pix_coords_pyramid[scale]\n",
    "        rotation = pose[:, :3, :3]\n",
    "        translation = pose[:, :3, -1].unsqueeze(1)\n",
    "\n",
    "        # Translation skew-matrix\n",
    "        t_skew = torch.zeros_like(translation)\n",
    "        t_skew = t_skew.repeat(1, translation.shape[-1], 1)\n",
    "        t_skew[:, 0, 1] = -translation[:, 0, 2]\n",
    "        t_skew[:, 0, 2] = translation[:, 0, 1]\n",
    "        t_skew[:, 1, 2] = -translation[:, 0, 0]\n",
    "        t_skew = t_skew - t_skew.transpose(1, 2)\n",
    "\n",
    "        # Move pixels with flow\n",
    "        flattened_flow = flow.view(flow.shape[0], flow.shape[1], -1)\n",
    "        pix_flow = torch.ones_like(pix_coords)\n",
    "        pix_flow[:, :2] = pix_coords[:, :2] + flattened_flow\n",
    "\n",
    "        # Epipolar geometry using the predicted flow as target image\n",
    "        if pix_group_size > 1:\n",
    "            pix_losses = 0.\n",
    "            for idx in range(0, pix_coords.shape[-1], pix_group_size):\n",
    "                pix_coords_i = pix_coords[..., idx:idx + pix_group_size]\n",
    "                poseT_mm_invT = pix_coords_i.transpose(1, 2) @ inv_intrinsics[:, :3, :3].transpose(1, 2)\n",
    "                rot_tskew_inv = rotation @ t_skew @ inv_intrinsics[:, :3, :3]\n",
    "                pix_losses += (poseT_mm_invT @ rot_tskew_inv @ pix_flow[..., idx:idx + pix_group_size]\n",
    "                               ).abs().sum()\n",
    "        else:\n",
    "            pix_losses = (pix_coords.transpose(1, 2) @ inv_intrinsics[:, :3, :3].transpose(1, 2) @\n",
    "                          rotation @ t_skew @ inv_intrinsics[:, :3, :3] @ pix_flow\n",
    "                          ).abs().sum()\n",
    "\n",
    "        if self.reduction == 'sum':\n",
    "            if pix_group_size > 1:\n",
    "                return pix_losses\n",
    "            else:\n",
    "                return pix_losses.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            if pix_group_size > 1:\n",
    "                return pix_losses / pix_coords.shape[-1]\n",
    "            else:\n",
    "                return pix_losses.mean()\n",
    "        elif self.reduction == 'none':\n",
    "            return pix_losses\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def adaptive_photometric_loss(\n",
    "            self,\n",
    "            img_src: torch.Tensor,\n",
    "            img_tgt: torch.Tensor,\n",
    "            depth: torch.Tensor,\n",
    "            flow: torch.Tensor,\n",
    "            pose: torch.Tensor,\n",
    "            intrinsics: torch.Tensor,\n",
    "            inv_intrinsics: torch.Tensor,\n",
    "            scale: int\n",
    "    ):\n",
    "        self.__set_batch_size(pose.shape[0], pose)\n",
    "        r = self.ssim_r\n",
    "        pix_coords = self.pix_coords_pyramid[scale]\n",
    "\n",
    "        # 3D warp target image\n",
    "        cam_coord_src = self.back_project(depth, inv_intrinsics, scale)\n",
    "        pix_coords_tgt = self.project_3d(cam_coord_src, intrinsics, pose, scale)\n",
    "        warped_tgt_3d = F.grid_sample(img_src, pix_coords_tgt, padding_mode='border', align_corners=True)\n",
    "\n",
    "        # Flow warp target image\n",
    "        pix_coords_i = pix_coords[:, :2]\\\n",
    "            .view(self.batch_size, 2, self.height // (2 ** scale), self.width // (2 ** scale)).contiguous()\n",
    "        pix_flow = pix_coords_i + flow\n",
    "        warped_tgt_flow = F.grid_sample(img_src, pix_flow.permute(0, 2, 3, 1).contiguous(),\n",
    "                                        padding_mode='border', align_corners=True)\n",
    "\n",
    "        # Pixel-wise minimum of SSIM maps\n",
    "        ssim_3d = self.ssim_f_3d(img_tgt, warped_tgt_3d).mean(1, keepdim=True)\n",
    "        ssim_flow = self.ssim_f_flow(img_tgt, warped_tgt_flow).mean(1, keepdim=True)\n",
    "\n",
    "        l1_3d = (img_tgt - warped_tgt_3d).abs().mean(1, keepdim=True)\n",
    "        l1_flow = (img_tgt - warped_tgt_flow).abs().mean(1, keepdim=True)\n",
    "\n",
    "        s_3d = r * (1 - ssim_3d) / 2 + (1 - r) * l1_3d\n",
    "        s_flow = r * (1 - ssim_flow) / 2 + (1 - r) * l1_flow\n",
    "\n",
    "        apc_loss = torch.stack([s_3d, s_flow], dim=1)\n",
    "        apc_loss = apc_loss.min(dim=1)[0]\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return apc_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return apc_loss.sum()\n",
    "        elif self.reduction == 'none':\n",
    "            return apc_loss\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def fwd_bwd_flow_consistency(self, flow_fwd: torch.Tensor, flow_bwd: torch.Tensor, scale: int):\n",
    "        self.__set_batch_size(flow_fwd.shape[0], flow_fwd)\n",
    "        alpha = self.flow_cons_params[0]\n",
    "        beta = self.flow_cons_params[1]\n",
    "\n",
    "        # Warp\n",
    "        bwd2fwd = F.grid_sample(flow_bwd, flow_fwd.permute(0, 2, 3, 1), padding_mode='border', align_corners=True)\n",
    "        fwd2bwd = F.grid_sample(flow_fwd, flow_bwd.permute(0, 2, 3, 1), padding_mode='border', align_corners=True)\n",
    "\n",
    "        # Consistency error\n",
    "        diff_fwd = (bwd2fwd + flow_fwd).abs()\n",
    "        diff_bwd = (fwd2bwd + flow_bwd).abs()\n",
    "\n",
    "        # Condition\n",
    "        bound_fwd = beta * (2 ** scale) * flow_fwd.norm(p=2, dim=1, keepdim=True)\n",
    "        with torch.no_grad():\n",
    "            bound_fwd = bound_fwd.clamp_min(alpha)\n",
    "\n",
    "        bound_bwd = beta * (2 ** scale) * flow_bwd.norm(p=2, dim=1, keepdim=True)\n",
    "        with torch.no_grad():\n",
    "            bound_bwd = bound_bwd.clamp_min(alpha)\n",
    "\n",
    "        # Mask\n",
    "        noc_mask_src = ((2 ** scale) * diff_bwd.norm(p=2, dim=1, keepdim=True) < bound_bwd)\n",
    "        noc_mask_tgt = ((2 ** scale) * diff_fwd.norm(p=2, dim=1, keepdim=True) < bound_fwd)\n",
    "\n",
    "        # Consistency loss\n",
    "        loss_fwd = (diff_fwd.mean(dim=1, keepdim=True) * noc_mask_tgt).sum() / noc_mask_tgt.sum()\n",
    "        loss_bwd = (diff_bwd.mean(dim=1, keepdim=True) * noc_mask_src).sum() / noc_mask_src.sum()\n",
    "        consistency_loss = (loss_fwd + loss_bwd) / 2\n",
    "        return consistency_loss"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1668508112228
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modules"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Depth decoder"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Modified from https://github.com/nianticlabs/monodepth2/blob/master/networks/depth_decoder.py\n",
    "and https://github.com/nianticlabs/monodepth2/blob/master/layers.py\n",
    "\"\"\"\n",
    "\n",
    "# Copyright Niantic 2019. Patent Pending. All rights reserved.\n",
    "#\n",
    "# This software is licensed under the terms of the Monodepth2 licence\n",
    "# which allows for non-commercial use only, the full terms of which are made\n",
    "# available in the LICENSE file.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Layer to perform a convolution followed by ELU\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        self.conv = Conv3x3(in_channels, out_channels)\n",
    "        self.nonlin = nn.ELU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.nonlin(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Conv3x3(nn.Module):\n",
    "    \"\"\"Layer to pad and convolve input\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, use_refl=True):\n",
    "        super(Conv3x3, self).__init__()\n",
    "\n",
    "        if use_refl:\n",
    "            self.pad = nn.ReflectionPad2d(1)\n",
    "        else:\n",
    "            self.pad = nn.ZeroPad2d(1)\n",
    "        self.conv = nn.Conv2d(int(in_channels), int(out_channels), 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.pad(x)\n",
    "        out = self.conv(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def upsample(x, size=None):\n",
    "    \"\"\"Upsample input tensor by a factor of 2\n",
    "    \"\"\"\n",
    "    if size is not None:\n",
    "        return F.interpolate(x, size=size, mode=\"nearest\")\n",
    "    else:\n",
    "        return F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "\n",
    "\n",
    "class DepthDecoder(nn.Module):\n",
    "    def __init__(self, num_ch_enc, scales=range(4), num_output_channels=1, use_skips=True):\n",
    "        super(DepthDecoder, self).__init__()\n",
    "\n",
    "        self.num_output_channels = num_output_channels\n",
    "        self.use_skips = use_skips\n",
    "        self.upsample_mode = 'nearest'\n",
    "        self.scales = scales\n",
    "\n",
    "        self.num_ch_enc = num_ch_enc\n",
    "        self.num_ch_dec = np.array([16, 32, 64, 128, 256])\n",
    "\n",
    "        # decoder\n",
    "        self.convs = OrderedDict()\n",
    "        for i in range(4, -1, -1):\n",
    "            # upconv_0\n",
    "            num_ch_in = self.num_ch_enc[-1] if i == 4 else self.num_ch_dec[i + 1]\n",
    "            num_ch_out = self.num_ch_dec[i]\n",
    "            self.convs[(\"upconv\", i, 0)] = ConvBlock(num_ch_in, num_ch_out)\n",
    "\n",
    "            # upconv_1\n",
    "            num_ch_in = self.num_ch_dec[i]\n",
    "            if self.use_skips and i > 0:\n",
    "                num_ch_in += self.num_ch_enc[i - 1]\n",
    "            num_ch_out = self.num_ch_dec[i]\n",
    "            self.convs[(\"upconv\", i, 1)] = ConvBlock(num_ch_in, num_ch_out)\n",
    "\n",
    "        for s in self.scales:\n",
    "            self.convs[(\"dispconv\", s)] = Conv3x3(self.num_ch_dec[s], self.num_output_channels)\n",
    "\n",
    "        self.decoder = nn.ModuleList(list(self.convs.values()))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_features):\n",
    "        self.outputs = {}\n",
    "\n",
    "        # decoder\n",
    "        x = input_features[-1]\n",
    "        for i in range(4, -1, -1):\n",
    "            x = self.convs[(\"upconv\", i, 0)](x)\n",
    "            if i != 0:\n",
    "                x = [upsample(x, input_features[i - 1].shape[-2:])]\n",
    "            else:\n",
    "                x = [upsample(x)]\n",
    "            if self.use_skips and i > 0:\n",
    "                x += [input_features[i - 1]]\n",
    "            x = torch.cat(x, 1)\n",
    "            x = self.convs[(\"upconv\", i, 1)](x)\n",
    "            if i in self.scales:\n",
    "                self.outputs[(\"disp\", i)] = self.sigmoid(self.convs[(\"dispconv\", i)](x))\n",
    "\n",
    "        return self.outputs"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1668508112428
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Flow decoder"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Modified from https://github.com/yzcjtr/GeoNet/blob/master/geonet_nets.py\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Layer to perform a convolution followed by ELU\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        self.conv = Conv3x3(in_channels, out_channels)\n",
    "        self.nonlin = nn.ELU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.nonlin(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Conv3x3(nn.Module):\n",
    "    \"\"\"Layer to pad and convolve input\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, use_refl=True):\n",
    "        super(Conv3x3, self).__init__()\n",
    "\n",
    "        if use_refl:\n",
    "            self.pad = nn.ReflectionPad2d(1)\n",
    "        else:\n",
    "            self.pad = nn.ZeroPad2d(1)\n",
    "        self.conv = nn.Conv2d(int(in_channels), int(out_channels), 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.pad(x)\n",
    "        out = self.conv(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def upsample(x, size=None):\n",
    "    \"\"\"Upsample input tensor by a factor of 2\n",
    "    \"\"\"\n",
    "    if size is not None:\n",
    "        return F.interpolate(x, size=size, mode=\"nearest\")\n",
    "    else:\n",
    "        return F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "\n",
    "\n",
    "class FlowDecoder(nn.Module):\n",
    "    def __init__(self, num_ch_enc, scales=range(4), num_output_frames=1, use_skips=True):\n",
    "        super(FlowDecoder, self).__init__()\n",
    "\n",
    "        self.num_output_frames = num_output_frames\n",
    "        self.use_skips = use_skips\n",
    "        self.upsample_mode = 'nearest'\n",
    "        self.scales = scales\n",
    "\n",
    "        self.num_ch_enc = num_ch_enc\n",
    "        self.num_ch_dec = np.array([16, 32, 64, 128, 256])\n",
    "\n",
    "        self.flow_scale = 0.1\n",
    "\n",
    "        # decoder\n",
    "        self.convs = OrderedDict()\n",
    "        for i in range(4, -1, -1):\n",
    "            # upconv_0\n",
    "            num_ch_in = self.num_ch_enc[-1] if i == 4 else self.num_ch_dec[i + 1]\n",
    "            num_ch_out = self.num_ch_dec[i]\n",
    "            self.convs[(\"upconv\", i, 0)] = ConvBlock(num_ch_in, num_ch_out)\n",
    "\n",
    "            # upconv_1\n",
    "            num_ch_in = self.num_ch_dec[i]\n",
    "            if self.use_skips and i > 0:\n",
    "                num_ch_in += self.num_ch_enc[i - 1]\n",
    "            num_ch_out = self.num_ch_dec[i]\n",
    "            self.convs[(\"upconv\", i, 1)] = ConvBlock(num_ch_in, num_ch_out)\n",
    "\n",
    "        for s in self.scales:\n",
    "            self.convs[(\"flowconv\", s)] = Conv3x3(self.num_ch_dec[s], 2 * self.num_output_frames)\n",
    "\n",
    "        self.decoder = nn.ModuleList(list(self.convs.values()))\n",
    "\n",
    "    def forward(self, input_features):\n",
    "        self.outputs = {}\n",
    "\n",
    "        # decoder\n",
    "        x = input_features[-1]\n",
    "        for i in range(4, -1, -1):\n",
    "            if i != 0:\n",
    "                x = upsample(x, input_features[i - 1].shape[-2:])\n",
    "            else:\n",
    "                x = upsample(x)\n",
    "            x = [self.convs[(\"upconv\", i, 0)](x)]\n",
    "            if self.use_skips and i > 0:\n",
    "                x += [input_features[i - 1]]\n",
    "            x = torch.cat(x, 1)\n",
    "            x = self.convs[(\"upconv\", i, 1)](x)\n",
    "            if i in self.scales:\n",
    "                self.outputs[(\"flow\", i)] = self.flow_scale * self.convs[(\"flowconv\", i)](x)\n",
    "\n",
    "        return self.outputs"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1668508112610
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### PoseCNN"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "From https://github.com/nianticlabs/monodepth2/blob/master/networks/pose_cnn.py\n",
    "\"\"\"\n",
    "\n",
    "# Copyright Niantic 2019. Patent Pending. All rights reserved.\n",
    "#\n",
    "# This software is licensed under the terms of the Monodepth2 licence\n",
    "# which allows for non-commercial use only, the full terms of which are made\n",
    "# available in the LICENSE file.\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class PoseCNN(nn.Module):\n",
    "    def __init__(self, num_input_frames, intrinsics=False, num_output_frames=None):\n",
    "        super(PoseCNN, self).__init__()\n",
    "\n",
    "        self.num_input_frames = num_input_frames\n",
    "        self.num_output_frames = (num_input_frames - 1) if num_output_frames is None else num_output_frames\n",
    "        self.intrinsics = intrinsics\n",
    "        self.output_parameters = 6 if not intrinsics else 8\n",
    "\n",
    "        self.convs = {}\n",
    "        self.convs[0] = nn.Conv2d(3 * num_input_frames, 16, 7, 2, 3)\n",
    "        self.convs[1] = nn.Conv2d(16, 32, 5, 2, 2)\n",
    "        self.convs[2] = nn.Conv2d(32, 64, 3, 2, 1)\n",
    "        self.convs[3] = nn.Conv2d(64, 128, 3, 2, 1)\n",
    "        self.convs[4] = nn.Conv2d(128, 256, 3, 2, 1)\n",
    "        self.convs[5] = nn.Conv2d(256, 256, 3, 2, 1)\n",
    "        self.convs[6] = nn.Conv2d(256, 256, 3, 2, 1)\n",
    "\n",
    "        self.pose_conv = nn.Conv2d(256, self.output_parameters * self.num_output_frames, 1)\n",
    "\n",
    "        self.num_convs = len(self.convs)\n",
    "\n",
    "        self.relu = nn.ReLU(True)\n",
    "\n",
    "        self.net = nn.ModuleList(list(self.convs.values()))\n",
    "\n",
    "    def forward(self, out):\n",
    "\n",
    "        for i in range(self.num_convs):\n",
    "            out = self.convs[i](out)\n",
    "            out = self.relu(out)\n",
    "\n",
    "        out = self.pose_conv(out)\n",
    "        out = out.mean(3).mean(2)\n",
    "\n",
    "        out = 0.01 * out.view(-1, self.num_output_frames, 1, self.output_parameters)\n",
    "\n",
    "        axisangle = out[..., :3].squeeze(1)\n",
    "        if not self.intrinsics:\n",
    "            translation = out[..., 3:].squeeze(1)\n",
    "            return {'axisangle': axisangle, 'translation': translation}\n",
    "        else:\n",
    "            translation = out[..., 3:7].squeeze(1)\n",
    "            intrinsics = out[...,  7:].squeeze(1)\n",
    "            return {'axisangle': axisangle, 'translation': translation, 'intrinsics': intrinsics}\n"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1668508112804
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### ResNet encoder"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Modified from https://github.com/nianticlabs/monodepth2/blob/master/networks/resnet_encoder.py\n",
    "\"\"\"\n",
    "\n",
    "# Copyright Niantic 2019. Patent Pending. All rights reserved.\n",
    "#\n",
    "# This software is licensed under the terms of the Monodepth2 licence\n",
    "# which allows for non-commercial use only, the full terms of which are made\n",
    "# available in the LICENSE file.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "\n",
    "class ResNetMultiImageInput(models.ResNet):\n",
    "    \"\"\"Constructs a resnet model with varying number of input images.\n",
    "    Adapted from https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
    "    \"\"\"\n",
    "    def __init__(self, block, layers, num_classes=1000, num_input_images=1):\n",
    "        super(ResNetMultiImageInput, self).__init__(block, layers)\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            num_input_images * 3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def resnet_multiimage_input(num_layers, pretrained=False, num_input_images=1):\n",
    "    \"\"\"Constructs a ResNet model.\n",
    "    Args:\n",
    "        num_layers (int): Number of resnet layers. Must be 18 or 50\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        num_input_images (int): Number of frames stacked as input\n",
    "    \"\"\"\n",
    "    assert num_layers in [18, 50], \"Can only run with 18 or 50 layer resnet\"\n",
    "    blocks = {18: [2, 2, 2, 2], 50: [3, 4, 6, 3]}[num_layers]\n",
    "    block_type = {18: models.resnet.BasicBlock, 50: models.resnet.Bottleneck}[num_layers]\n",
    "    model = ResNetMultiImageInput(block_type, blocks, num_input_images=num_input_images)\n",
    "\n",
    "    if pretrained:\n",
    "        loaded = model_zoo.load_url(models.resnet.model_urls['resnet{}'.format(num_layers)])\n",
    "        loaded['conv1.weight'] = torch.cat(\n",
    "            [loaded['conv1.weight']] * num_input_images, 1) / num_input_images\n",
    "        model.load_state_dict(loaded)\n",
    "    return model\n",
    "\n",
    "\n",
    "class ResnetEncoder(nn.Module):\n",
    "    \"\"\"Pytorch module for a resnet encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, pretrained, num_input_images=1):\n",
    "        super(ResnetEncoder, self).__init__()\n",
    "\n",
    "        self.num_ch_enc = np.array([64, 64, 128, 256, 512])\n",
    "\n",
    "        resnets = {18: models.resnet18,\n",
    "                   34: models.resnet34,\n",
    "                   50: models.resnet50,\n",
    "                   101: models.resnet101,\n",
    "                   152: models.resnet152}\n",
    "\n",
    "        # weights = {18: models.ResNet18_Weights.DEFAULT,\n",
    "        #            34: models.ResNet34_Weights.DEFAULT,\n",
    "        #            50: models.ResNet50_Weights.IMAGENET1K_V2,\n",
    "        #            101: models.ResNet101_Weights.IMAGENET1K_V2,\n",
    "        #            152: models.ResNet152_Weights.IMAGENET1K_V2}\n",
    "\n",
    "        if num_layers not in resnets:\n",
    "            raise ValueError(\"{} is not a valid number of resnet layers\".format(num_layers))\n",
    "\n",
    "        if num_input_images > 1:\n",
    "            self.encoder = resnet_multiimage_input(num_layers, pretrained, num_input_images)\n",
    "        else:\n",
    "            # self.encoder = resnets[num_layers](weights=weights[num_layers])\n",
    "            self.encoder = resnets[num_layers](pretrained=True)\n",
    "\n",
    "        if num_layers > 34:\n",
    "            self.num_ch_enc[1:] *= 4\n",
    "\n",
    "    def forward(self, input_image, norm_input=False):\n",
    "        self.features = []\n",
    "        if norm_input:\n",
    "            x = (input_image - 0.45) / 0.225\n",
    "        else:\n",
    "            x = input_image\n",
    "        x = self.encoder.conv1(x)\n",
    "        x = self.encoder.bn1(x)\n",
    "        self.features.append(self.encoder.relu(x))\n",
    "        self.features.append(self.encoder.layer1(self.encoder.maxpool(self.features[-1])))\n",
    "        self.features.append(self.encoder.layer2(self.features[-1]))\n",
    "        self.features.append(self.encoder.layer3(self.features[-1]))\n",
    "        self.features.append(self.encoder.layer4(self.features[-1]))\n",
    "\n",
    "        return self.features"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1668508112999
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GLNet model "
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from itertools import combinations\n",
    "\n",
    "class GLNet:\n",
    "    def __init__(\n",
    "            self,\n",
    "            pose_input_num: int,\n",
    "            pose_output_num: int,\n",
    "            depth_input_num: int,\n",
    "            depth_output_num: int,\n",
    "            flow_input_num: int,\n",
    "            flow_output_num: int,\n",
    "            loss_parameters: dict,\n",
    "            pred_intrinsics: bool = False,\n",
    "            depth_res_layers: int = 18,\n",
    "            depth_use_skips: bool = True,\n",
    "            flow_res_layers: int = 18,\n",
    "            flow_use_skips: int = True,\n",
    "            resnet_pretrained: bool = True,\n",
    "            shared_resnet: bool = True,\n",
    "            depth_limits: tuple = (0.1, 100.),\n",
    "            frame_ids: list = None,\n",
    "            scales: int = 4\n",
    "    ):\n",
    "        self.shared_resnet = shared_resnet\n",
    "        self.intrinsics = pred_intrinsics\n",
    "\n",
    "        self.pose_input_num = pose_input_num\n",
    "        self.depth_input_num = depth_input_num\n",
    "        self.flow_input_num = flow_input_num\n",
    "\n",
    "        self.scales = scales\n",
    "\n",
    "        self.frame_ids = np.sort(frame_ids) if frame_ids is not None else [0, 1]\n",
    "        self.depth_limits = depth_limits\n",
    "\n",
    "        if shared_resnet and \\\n",
    "                (depth_input_num != flow_input_num or depth_res_layers != flow_res_layers):\n",
    "            raise AttributeError(\"In case of shared resnet the flow and depth input nums must match, \"\n",
    "                                 \"as well as the num of res layers!\")\n",
    "\n",
    "        self.camera_net = PoseCNN(pose_input_num, pred_intrinsics, pose_output_num)\n",
    "\n",
    "        self.depth_encoder = ResnetEncoder(depth_res_layers, resnet_pretrained, depth_input_num)\n",
    "        self.depth_decoder = DepthDecoder(self.depth_encoder.num_ch_enc, range(scales),\n",
    "                                          depth_output_num, depth_use_skips)\n",
    "\n",
    "        if self.shared_resnet:\n",
    "            self.flow_encoder = self.depth_encoder\n",
    "        else:\n",
    "            self.flow_encoder = ResnetEncoder(flow_res_layers, resnet_pretrained, flow_input_num)\n",
    "\n",
    "        self.flow_decoder = FlowDecoder(self.flow_encoder.num_ch_enc, range(scales),\n",
    "                                        flow_output_num, flow_use_skips)\n",
    "\n",
    "        self.glnet_loss = GLNetLoss(**loss_parameters)\n",
    "        \n",
    "        parameter_list = [\n",
    "            {'params': self.depth_decoder.parameters()}, \n",
    "            {'params': self.depth_encoder.parameters()}, \n",
    "            {'params': self.camera_net.parameters()}\n",
    "        ]\n",
    "        if not self.shared_resnet:\n",
    "            parameter_list += [{'params': self.flow_encoder.parameters()}]\n",
    "        self.optimizer = torch.optim.Adam(parameter_list, lr=2e-4, betas=(0.9, 0.999))\n",
    "    \n",
    "    def train(self):\n",
    "        self.depth_decoder.train()\n",
    "        self.depth_encoder.train()\n",
    "        self.flow_decoder.train()\n",
    "        if not self.shared_resnet:\n",
    "            self.flow_encoder.train()\n",
    "        self.camera_net.train()\n",
    "\n",
    "    def eval(self):\n",
    "        self.depth_decoder.eval()\n",
    "        self.depth_encoder.eval()\n",
    "        self.flow_decoder.eval()\n",
    "        if not self.shared_resnet:\n",
    "            self.flow_encoder.eval()\n",
    "        self.camera_net.eval()\n",
    "\n",
    "    def cuda(self):\n",
    "        self.depth_decoder.cuda()\n",
    "        self.depth_encoder.cuda()\n",
    "        self.flow_decoder.cuda()\n",
    "        if not self.shared_resnet:\n",
    "            self.flow_encoder.cuda()\n",
    "        self.camera_net.cuda()\n",
    "\n",
    "    def cpu(self):\n",
    "        self.depth_decoder.cpu()\n",
    "        self.depth_encoder.cpu()\n",
    "        self.flow_decoder.cpu()\n",
    "        if not self.shared_resnet:\n",
    "            self.flow_encoder.cpu()\n",
    "        self.camera_net.cpu()\n",
    "\n",
    "    def training_step(self, batch: dict, *args, **kwargs):\n",
    "        self.train()\n",
    "\n",
    "        all_color_aug = torch.stack([batch[('color_aug', i, 0)] for i in self.frame_ids], dim=1)\n",
    "        disps, poses, flows_fwd, flows_bwd = self.__predict_for_train_val(all_color_aug)\n",
    "\n",
    "        # Convert disparities to depths\n",
    "        depths = {}\n",
    "        for frame, disp in disps.items():\n",
    "            depth_dict = {}\n",
    "            for scale in range(self.scales):\n",
    "                depth_dict[('depth', scale)] = \\\n",
    "                    disp2depth(disp[('disp', scale)], self.depth_limits[0], self.depth_limits[1])[1]\n",
    "            depths[frame] = depth_dict\n",
    "\n",
    "        loss, loss_parts = self.glnet_loss(batch, depths, poses, flows_fwd, self.scales, disps, flows_bwd)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        loss_parts['total_loss'] = loss\n",
    "        return loss_parts\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validation_step(self, batch: dict, *args, **kwargs):\n",
    "        self.eval()\n",
    "\n",
    "        all_color = torch.stack([batch[('color', i, 0)] for i in self.frame_ids], dim=1)\n",
    "        disps, poses, flows_fwd, flows_bwd = self.__predict_for_train_val(all_color)\n",
    "\n",
    "        # Convert disparities to depths\n",
    "        depths = {}\n",
    "        for frame, disp in disps.items():\n",
    "            depth_dict = {}\n",
    "            for scale in range(self.scales):\n",
    "                depth_dict[('depth', scale)] = \\\n",
    "                    disp2depth(disp[('disp', scale)], self.depth_limits[0], self.depth_limits[1])[1]\n",
    "            depths[frame] = depth_dict\n",
    "\n",
    "        loss, loss_parts = self.glnet_loss(batch, depths, poses, flows_fwd, self.scales, disps, flows_bwd)\n",
    "        \n",
    "        loss_parts['total_loss'] = loss\n",
    "        return loss_parts\n",
    "\n",
    "    def __predict_for_train_val(self, image_stack: torch.Tensor, is_train: bool = True):\n",
    "        inp_shape = image_stack.shape\n",
    "        key_features = []\n",
    "\n",
    "        disps = {}\n",
    "        poses = {}\n",
    "        flows_fwd = {}\n",
    "        flows_bwd = {}\n",
    "\n",
    "        # Depth estimation\n",
    "        for f_idx in range(len(self.frame_ids) - self.depth_input_num + 1):\n",
    "            frame_ids = self.frame_ids[f_idx:f_idx + self.depth_input_num]\n",
    "            key_features.append(f_idx + (self.depth_input_num - 1) // 2)\n",
    "            disps[frame_ids[(self.depth_input_num - 1) // 2]] = \\\n",
    "                self.depth_decoder(\n",
    "                    self.depth_encoder(\n",
    "                        image_stack[:, f_idx:f_idx + self.depth_input_num]\n",
    "                        .view(inp_shape[0], -1, *inp_shape[-2:]).contiguous()\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # Pose estimation\n",
    "        for feature_group in combinations(key_features, self.pose_input_num):\n",
    "            fg_list = list(feature_group)\n",
    "            frame_group = self.frame_ids[fg_list]\n",
    "\n",
    "            poses[tuple(frame_group)] = self.camera_net(\n",
    "                image_stack[:, fg_list]\n",
    "                .view(inp_shape[0], -1, *inp_shape[-2:]).contiguous()\n",
    "            )\n",
    "\n",
    "        # Forward and Backward Flow estimation\n",
    "        for feature_group in combinations(key_features, self.flow_input_num):\n",
    "            fg_list = list(feature_group)\n",
    "            frame_group = self.frame_ids[fg_list]\n",
    "\n",
    "            flows_fwd[tuple(frame_group)] = \\\n",
    "                self.flow_decoder(\n",
    "                    self.flow_encoder(\n",
    "                        image_stack[:, fg_list]\n",
    "                        .view(inp_shape[0], -1, *inp_shape[-2:]).contiguous()\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            if is_train:\n",
    "                flows_bwd[tuple(frame_group)] = \\\n",
    "                    self.flow_decoder(\n",
    "                        self.flow_encoder(\n",
    "                            image_stack[:, fg_list[::-1]]\n",
    "                            .view(inp_shape[0], -1, *inp_shape[-2:]).contiguous()\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        return disps, poses, flows_fwd, flows_bwd\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1668508113201
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Config"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "ON_GPU = torch.cuda.is_available()\n",
    "\n",
    "DATA_CFG = {\n",
    "    'batch_size': 16,\n",
    "    'img_size': (240, 320),\n",
    "    'frame_idxs': [-1, 0, 1, 2],\n",
    "    'scales': 4,\n",
    "\n",
    "    'train_txt': os.path.join(os.getcwd(), \"train_files.txt\"),\n",
    "    'val_txt': os.path.join(os.getcwd(), \"val_files.txt\"),\n",
    "    'test_txt': None,\n",
    "    'num_workers': 6\n",
    "}\n",
    "\n",
    "\n",
    "GLNET_LOSS_CFG = {\n",
    "    'img_size': DATA_CFG['img_size'],\n",
    "    'scales': DATA_CFG['scales'],\n",
    "    'mvs_weight': 10.,\n",
    "    'epi_weight': 10.,\n",
    "    'apc_weight': 1.,\n",
    "    'disp_smooth': 0.5,\n",
    "    'flow_smooth': 0.2,\n",
    "    'flow_cons_params': (3.0, 0.05),\n",
    "    'flow_cons_weight': 0.2,\n",
    "    'ssim_r': 0.85,\n",
    "    'reduction': 'mean'\n",
    "}\n",
    "\n",
    "\n",
    "GLNET_CFG = {\n",
    "    'pose_input_num': 2,\n",
    "    'pose_output_num': 1,\n",
    "    'pred_intrinsics': False,\n",
    "    'resnet_pretrained': True,\n",
    "\n",
    "    'depth_input_num': 3,\n",
    "    'depth_output_num': 1,\n",
    "    'depth_res_layers': 18,\n",
    "    'depth_use_skips': True,\n",
    "\n",
    "    'flow_input_num': 2,\n",
    "    'flow_output_num': 1,\n",
    "    'flow_res_layers': 18,\n",
    "    'flow_use_skips': True,\n",
    "\n",
    "    'shared_resnet': False,\n",
    "    'frame_ids': DATA_CFG['frame_idxs'],\n",
    "    'scales': DATA_CFG['scales'],\n",
    "    'depth_limits': (0.1, 50.0),\n",
    "    'loss_parameters': GLNET_LOSS_CFG\n",
    "}\n",
    "\n",
    "\n",
    "TRAIN_CFG = {\n",
    "    'epochs': 10,\n",
    "    'eval_every_n_steps': 993,\n",
    "    'eval_ratio': 0.1\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1668508323773
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Main file"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "log_path = os.path.join(os.getcwd(), 'logs', datetime.now().strftime('%d%m%y_%H%M%S'))\n",
    "os.makedirs(log_path)\n",
    "# logger = SummaryWriter(log_path)\n",
    "logger = None\n",
    "\n",
    "\n",
    "def setup_dataloaders():\n",
    "    def readlines(filename):\n",
    "        \"\"\"Read all the lines in a text file and return as a list\n",
    "        \"\"\"\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.read().splitlines()\n",
    "        return lines\n",
    "\n",
    "    train_file_names = readlines(DATA_CFG['train_txt'])\n",
    "    val_file_names = readlines(DATA_CFG['val_txt'])\n",
    "\n",
    "    train_set = TartanAirDataset(\n",
    "        data_path=None,\n",
    "        filenames=train_file_names,\n",
    "        height=DATA_CFG['img_size'][0],\n",
    "        width=DATA_CFG['img_size'][1],\n",
    "        frame_idxs=DATA_CFG['frame_idxs'],\n",
    "        num_scales=DATA_CFG['scales'],\n",
    "        is_train=True\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_set,\n",
    "        batch_size=DATA_CFG['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=DATA_CFG['num_workers'],\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    val_set = TartanAirDataset(\n",
    "        data_path=None,\n",
    "        filenames=val_file_names,\n",
    "        height=DATA_CFG['img_size'][0],\n",
    "        width=DATA_CFG['img_size'][1],\n",
    "        frame_idxs=DATA_CFG['frame_idxs'],\n",
    "        num_scales=DATA_CFG['scales'],\n",
    "        is_train=False\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_set,\n",
    "        batch_size=DATA_CFG['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=DATA_CFG['num_workers'],\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "\n",
    "def validate(model, epoch, val_loader, partition=1.0, logger=None, step=None):\n",
    "    model.eval()\n",
    "    val_len = int(partition * len(val_loader))\n",
    "    val_loss = 0.\n",
    "    with tqdm(total=val_len, leave=False) as pbar:\n",
    "        for bidx, batch in enumerate(val_loader):\n",
    "            # Evaluate only a subset\n",
    "            if bidx > val_len:\n",
    "                break\n",
    "\n",
    "            # Data and model\n",
    "            if ON_GPU:\n",
    "                for key, value in batch.items():\n",
    "                    batch[key] = value.cuda()\n",
    "\n",
    "            val_losses = model.validation_step(batch)\n",
    "\n",
    "            # Overall loss\n",
    "            val_loss += val_losses['total_loss'].item()\n",
    "            \n",
    "            # Update progress bar to contain all loss elements\n",
    "            pbar_dict = {\n",
    "                'Epoch': f'{epoch+1}/{TRAIN_CFG[\"epochs\"]}',\n",
    "                'AvgLoss': val_loss / (bidx + 1)\n",
    "            }\n",
    "            for key, value in val_losses.items():\n",
    "                pbar_dict[key] = value.item()\n",
    "            pbar.set_postfix(pbar_dict)\n",
    "            pbar.update(1)\n",
    "    \n",
    "    if logger is not None:\n",
    "        logger.add_scalar('Validation/Avg Loss', val_loss / val_len, epoch if step is None else step)\n",
    "\n",
    "\n",
    "def main():\n",
    "    train_loader, val_loader = setup_dataloaders()\n",
    "\n",
    "    model = GLNet(**GLNET_CFG)\n",
    "    if ON_GPU:\n",
    "        model.cuda()\n",
    "\n",
    "    for epoch in range(TRAIN_CFG['epochs']):\n",
    "        model.train()\n",
    "        \n",
    "        # Training phase\n",
    "        with tqdm(total=len(train_loader), leave=False) as pbar:\n",
    "            epoch_loss = 0.\n",
    "            for bidx, batch in enumerate(train_loader):\n",
    "                # Data and model\n",
    "                if ON_GPU:\n",
    "                    for key, value in batch.items():\n",
    "                        batch[key] = value.cuda()\n",
    "\n",
    "                losses = model.training_step(batch)\n",
    "\n",
    "                # Validate every n step if needed\n",
    "                if (val_loader is not None) and \\\n",
    "                    (TRAIN_CFG['eval_every_n_steps'] is not None) and \\\n",
    "                    ((bidx + 1) % TRAIN_CFG['eval_every_n_steps'] == 0):\n",
    "                    validate(model, epoch, val_loader, TRAIN_CFG['eval_ratio'], logger, bidx)\n",
    "                    model.train()\n",
    "\n",
    "                # Log loss elements every step\n",
    "                if logger is not None:\n",
    "                    logger.add_scalars('Train/Losses', losses, epoch * len(train_loader) + bidx)\n",
    "\n",
    "                # Overall loss\n",
    "                epoch_loss += losses['total_loss'].item()\n",
    "                \n",
    "                # Update progress bar to contain loss elements\n",
    "                pbar_dict = {\n",
    "                    'Epoch': f'{epoch+1}/{TRAIN_CFG[\"epochs\"]}',\n",
    "                    'AvgLoss': epoch_loss / (bidx + 1)\n",
    "                }\n",
    "                for key, value in losses.items():\n",
    "                    pbar_dict[key] = value.item()\n",
    "                pbar.set_postfix(pbar_dict)\n",
    "                pbar.update(1)\n",
    "\n",
    "        if val_loader is not None:\n",
    "            validate(model, epoch, val_loader, TRAIN_CFG['eval_ratio'])\n",
    "\n",
    "        torch.save(\n",
    "            {\n",
    "            'model':{\n",
    "                'depth_decoder': model.depth_decoder.state_dict(),\n",
    "                'depth_encoder': model.depth_encoder.state_dict(),\n",
    "                'flow_decoder': model.flow_decoder.state_dict(),\n",
    "                'flow_encoder': model.flow_encoder.state_dict(),\n",
    "                'camera_net': model.camera_net.state_dict()},\n",
    "            'optimizer': model.optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'train_loss': epoch_loss\n",
    "            },\n",
    "            os.path.join(log_path, f'model_epoch{epoch}.pth')\n",
    "        )\n",
    "\n",
    "main()\n"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1668508182889
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
